{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Anomaly Detection and Interpretation from Tabular Data Using Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present a novel approach to anomaly detection and interpretation from tabular data using a transformer\n",
    "architecture. Our approach comprises two main components: the Anomaly Detector and the Anomaly Interpreter. \n",
    "\n",
    "#### Anomaly Detector:\n",
    "The Anomaly Detector component functions as a supervised classification model, comprising a multi-layer bidirectional Transformer encoder and a subsequent binary classification layer. This model is trained using labeled tabular data to optimize performance, supported by advanced features such as a custom tokenizer and specialized embedding layers (token and positional embeddings) to enhance data understanding and processing.\n",
    "\n",
    "Custom Tokenizer: We have developed a custom tokenizer specifically designed to prepare textual representations of tabular data for processing by transformer models. This tokenizer focuses on tokenizing, vocabulary mapping, and handling out-of-vocabulary items.\n",
    "\n",
    "Embedding Layer: This layer converts each token from its tokenized form (i.e., discrete IDs) into a vector of fixed size (i.e., continuous vector representation). The embedding layer combines three types of embeddings to represent each token fully before it is processed by the subsequent layers of the model. \n",
    "\n",
    "Encoder Layers: The primary objective of encoder layers is to develop a deep, nuanced understanding of language context and semantics. Each layer processes input embeddings (combined token and positional embeddings) to build increasingly complex representations. \n",
    "\n",
    "Classification Layer: Pooling Layer. The output of the last encoder layer (corresponding to each token) is pooled to create a fixed-size output. Dense Layer. This is a fully connected neural network layer that maps the high-dimensional output of previous layer to two classes in our binary classification problem\n",
    "\n",
    "#### Anomaly Interpreter: \n",
    "The objective of this component is to interpret anomalies within textual data transformed from tabular format using the trained transformer model.\n",
    "\n",
    "Association Matrix: An association matrix ùëÄ captures the average attention between all pairs of tokens across\n",
    "the normal training data. \n",
    "\n",
    "Extract Attention Weights: This module is designed to extract attention weights from the trained transformer model for a given input row.\n",
    "\n",
    "Identify Closely Related Tokens: This module aims to find pairs of tokens in an input row that receive high attention weights, indicating a strong association according to the model‚Äôs attention mechanism.\n",
    "\n",
    "Identify Violation Rules: After identifying high-attention pairs in a test row deemed anomalous, this module compare these pairs against the association matrix ùëÄ to identify rule violations based on a threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1713377863406
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uriel/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Convert Tabular Data to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address unique tabular data challenges, we developed a methodology to convert table rows of any data type into a sentence-like structure, enabling effective data processing by the transformer model. \n",
    "\n",
    "By converting each row of a CSV file into a sentence formatted as [column name: value], the tokenizer ensures that numeric values and their associated column names are treated as single tokens, thus preventing the splitting of numerical values during tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1713377863789
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#source: https://odds.cs.stonybrook.edu/breast-cancer-wisconsin-original-dataset/\n",
    "file_path = 'breast-cancer-wisconsin.csv'\n",
    "\n",
    "df = pd.read_csv(file_path).drop('ID', axis=1) # Assuming the dataset has an 'ID' column\n",
    "# df = pd.read_csv(file_path) \n",
    "\n",
    "def row_to_sentence(row):\n",
    "    return ','.join([f'[{col}:{val}]' for col, val in row.items()])\n",
    "\n",
    "df['sentence'] = df.drop('Class',axis=1).apply(row_to_sentence, axis=1)\n",
    "sentences = df['sentence'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Construct Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vocabulary comprises the complete set of unique tokens that the model is designed to recognize and process. Each token in the vocabulary is assigned a unique identifier, which facilitates the conversion of textual data into a numerical form that the transformer model can comprehend. In this step, we construct a comprehensive vocabulary that uniquely identifies every token encountered in the dataset. By parsing through each sentence and extracting unique tokens, we ensure that each token is represented by a unique integer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1713377864160
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def generate_vocab(df):\n",
    "    vocab = {}\n",
    "    column_values = {}\n",
    "    for col in df.columns:\n",
    "        unique_values = set(df[col].unique())\n",
    "        column_values[col] = {str(val): None for val in unique_values}  \n",
    "\n",
    "    token_id = 0\n",
    "    for col, values in column_values.items():\n",
    "        for val in values:\n",
    "            token = f'[{col}:{val}]'\n",
    "            vocab[token] = token_id\n",
    "            token_id += 1\n",
    "\n",
    "    # Special tokens\n",
    "    vocab[\"<OOV>\"] = token_id\n",
    "    vocab[\"<PAD>\"] = token_id + 1\n",
    "\n",
    "    return vocab, column_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1713377864432
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "    vocab, column_values=generate_vocab(df)\n",
    "    vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Custom Tokenizer\n",
    "This code implements a custom tokenizer specifically designed to prepare textual representations of tabular data for processing with transformer models, with an emphasis on customized embeddings. We have developed a custom tokenizer specifically designed to prepare textual representations of tabular data for processing by transformer models. This tokenizer focuses on tokenizing, vocabulary mapping, and handling out-of-vocabulary items.\n",
    "\n",
    "The inclusion of out-of-vocabulary (OOV) tokens is crucial for handling words that were not present in the training dataset, ensuring the model can process any text seamlessly. For numeric tabular data, a specific method is developed to manage OOV tokens by assessing the \"closeness\" of numeric values associated with column names (e.g., col:val). If an OOV token is numeric, its closeness to known tokens is calculated by locating the nearest numeric value for that column in the training data and using that token‚Äôs ID. A numeric closeness function, which operates based on a threshold value for closeness set based on the dataset,aids in mapping an OOV token to the nearest known token. If no value meets the threshold, the function returns a special <OOV> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1713377864702
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def find_closest_known_token(col_val_pair, column_values, threshold=0.5):\n",
    "    print(col_val_pair)\n",
    "    col, val = col_val_pair.strip('[]').split(':')\n",
    "    val = float(val)  \n",
    "    \n",
    "    closest_val = None\n",
    "    closest_diff = float('inf')\n",
    "    for known_val in column_values[col]:\n",
    "        known_val_float = float(known_val)\n",
    "        diff = abs(known_val_float - val)\n",
    "        if diff < closest_diff and diff <= threshold:\n",
    "            closest_val = known_val\n",
    "            closest_diff = diff\n",
    "\n",
    "    if closest_val is not None:\n",
    "        return f'[{col}:{closest_val}]'\n",
    "    else:\n",
    "        return \"<OOV>\"\n",
    "\n",
    "\n",
    "def custom_tokenizer(examples, return_tensors=\"pt\"):\n",
    "    tokenized_outputs = []\n",
    "    for sentence in examples:\n",
    "        tokens = sentence.split(',')\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                token_id = vocab[token]\n",
    "            else:\n",
    "                closest_token = find_closest_known_token(token, column_values)\n",
    "                token_id = vocab.get(closest_token, vocab[\"<OOV>\"])\n",
    "            token_ids.append(token_id)\n",
    "        tokenized_outputs.append(token_ids)\n",
    "\n",
    "    return {\"input_ids\": tokenized_outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1713377864974
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "id_to_token = {id: token for token, id in vocab.items()}\n",
    "\n",
    "def convert_ids_to_tokens(token_ids):\n",
    "    return [id_to_token.get(id, \"<OOV>\") for id in token_ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Train-Test data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1713377865592
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "majority_value = df[\"Class\"].value_counts().idxmax() \n",
    "df['Class'] = df['Class'].apply(lambda x: 1 if x == majority_value else 0)\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(df['sentence'], df['Class'], test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': train_sentences.tolist(), 'labels': train_labels.tolist()})\n",
    "val_dataset = Dataset.from_dict({'text': val_sentences.tolist(), 'labels': val_labels.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1713377866704
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/614 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda examples: custom_tokenizer(examples['text']), batched=True)\n",
    "val_dataset = val_dataset.map(lambda examples: custom_tokenizer(examples['text']), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1713377867247
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Custom Trainer\n",
    "We develop a custom token embedding layer that initializes the embedding lookup table based on the vocabulary size and the embedding dimension. For the model architecture we used, the embedding dimension is predefined to be 768.\n",
    "A tensor of custom embeddings is created, containing randomly initialized weights. The objective of incorporating custom embeddings is to leverage domain-specific knowledge or specific representations of the tabular data, which might not be captured by the pre-trained embeddings. \n",
    "\n",
    "Here's a breakdown of the key components and objectives:\n",
    "\n",
    "**Custom Embeddings Initialization**: The script begins by defining the size of the custom embeddings based on the vocabulary size (vocab_size) and specifying the embedding dimension (embedding_dim). A tensor of custom embeddings is created, containing randomly initialized weights. This step is crucial for tailoring the model to better understand the specific dataset it will be trained on, especially when dealing with specialized vocabulary.\n",
    "\n",
    "**Model Setup**: The BERT model is loaded with pre-trained weights from 'distilbert-base-uncased', 'bert-base-uncased', 'bert-large-uncased', 'roberta-base', or 'albert-base-v2'. This is then modified to replace the model's original word embeddings with the newly initialized custom embeddings. This replacement allows the model to utilize the custom embeddings during training and inference, potentially enhancing its ability to capture the nuances of the dataset.\n",
    "\n",
    "**Freezing Custom Embeddings** (optional): We freeze the custom embeddings by setting their requires_grad attribute to False. This prevents the embeddings from being updated during the training process, which can be desirable if the custom embeddings already capture the necessary information about the dataset and the goal is to preserve these representations.\n",
    "\n",
    "**Training and Optimization**: The script configures the model for training on a specified device (GPU, if available) and defines an optimizer (AdamW) with a learning rate for updating the model's weights. It then enters a training loop where it processes batches of data, calculates loss, performs backpropagation, and updates the model's weights. This process is iterated over a predefined number of epochs, with the total loss being reported after each epoch.\n",
    "\n",
    "**Evaluation**: After training, the model is evaluated on a validation set to estimate its accuracy. The evaluation loop feeds batches of validation data to the model, computes predictions, and compares them to the true labels to calculate accuracy. This step provides a simplified metric for assessing the model's performance.\n",
    "\n",
    "\n",
    "_Note_: When using PyTorch's DataLoader directly for training and validation, instead of relying on Hugging Face's Trainer, we implement a custom training loop. This custom loop iterates over the train_loader and val_loader, feed the batches to the model, calculate the loss, and update the model parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1713377889272
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/_rsvm6v54878sdz1mhf4qng00000gp/T/ipykernel_13233/2485589983.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  custom_embeddings_tensor = torch.tensor(custom_embeddings, dtype=torch.float)\n",
      "You are using a model of type albert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.weight', 'classifier.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'pooler.dense.bias', 'pooler.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.8.attention.self.query.weight', 'classifier.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:11<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Total Loss: 2.4153285399079323\n",
      "Epoch 1 took: 11.25 seconds\n",
      "Total training time: 11.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:12<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Total Loss: 1.61954041197896\n",
      "Epoch 2 took: 12.19 seconds\n",
      "Total training time: 23.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:14<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished. Total Loss: 1.2605879046022892\n",
      "Epoch 3 took: 14.81 seconds\n",
      "Total training time: 38.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "import time\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "#################\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 768  # Adjust as needed\n",
    "# embedding_dim = 1024\n",
    "custom_embeddings = torch.FloatTensor(np.random.rand(vocab_size, embedding_dim))\n",
    "#################\n",
    "\n",
    "custom_embeddings_tensor = torch.tensor(custom_embeddings, dtype=torch.float)\n",
    "\n",
    "# Load the model\n",
    "# model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2, output_attentions=True)\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, output_attentions=True)\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2, output_attentions=True)\n",
    "# model = BertForSequenceClassification.from_pretrained('roberta-base', num_labels=2, output_attentions=True)\n",
    "model = BertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2, output_attentions=True)\n",
    "\n",
    "model.bert.embeddings.word_embeddings = torch.nn.Embedding(num_embeddings=custom_embeddings_tensor.size(0), embedding_dim=custom_embeddings_tensor.size(1))\n",
    "model.bert.embeddings.word_embeddings.weight = torch.nn.Parameter(custom_embeddings_tensor)\n",
    "\n",
    "# Optionally, freeze the custom embeddings to prevent them from being updated during training\n",
    "model.bert.embeddings.word_embeddings.weight.requires_grad = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "for epoch in range(3):  # number of epochs\n",
    "    epoch_start_time = time.time() \n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_end_time = time.time() \n",
    "    epoch_time = epoch_end_time - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} finished. Total Loss: {total_loss}\")\n",
    "    print(f\"Epoch {epoch + 1} took: {epoch_time:.2f} seconds\")\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total training time: {total_time:.2f} seconds\") \n",
    "\n",
    "model.eval()\n",
    "total_eval_accuracy = 0\n",
    "for batch in tqdm(val_loader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    accuracy = (predictions == batch['labels']).cpu().numpy().mean()  \n",
    "    total_eval_accuracy += accuracy\n",
    "\n",
    "print(f\"Validation Accuracy: {total_eval_accuracy / len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 6: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1713377889648
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9710144927536232\n",
      "Test F1 Score: 0.9710144927536232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "for batch in tqdm(val_loader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "    batch_labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "    predictions.extend(batch_predictions)\n",
    "    true_labels.extend(batch_labels)\n",
    "\n",
    "bert_accuracy = accuracy_score(true_labels, predictions)\n",
    "# bert_f1 = f1_score(true_labels, predictions, average='binary')  # Use 'micro' or 'macro' for multi-class classification\n",
    "bert_f1 = f1_score(true_labels, predictions, average='micro')\n",
    "\n",
    "print(f\"Test Accuracy: {bert_accuracy}\")\n",
    "print(f\"Test F1 Score: {bert_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 7: Interpret the Model's Decision Using Attention\n",
    "### Interpretation Approach 1. Direct Usage of Attention Matrix\n",
    "The Anomaly Interpreter utilizes attention weights from the trained transformer model to analyze associations between token pairs in anomalous rows compared to a benchmark association matrix reflecting average token associations in normal data. Significant deviations in associations are flagged as rule violations, offering deep insights into the nature of the anomalies.\n",
    "\n",
    "\n",
    "### Extract the Attention Weights\n",
    "This module is designed to extract attention weights from the trained transformer model for a given input row. The module starts by tokenizing the input row using our custom tokenizer, wrapping the row in a list to match the tokenizer‚Äôs expected input format. \n",
    "\n",
    "Next, an attention mask is created for informing the model which parts of the input are actual data versus padding, ensuring that attention calculations are only performed on meaningful data. The module returns the predicted class, the probability of that class, and the attention weights. \n",
    "\n",
    "The attention weights is used by the next module to interpret which parts of the input row the model focused on when making its prediction. This module assumes that the last layer‚Äôs attention is the most representative of the model‚Äôs final decision-making process. In transformer-based models, the intuition is that earlier layers capture more about syntax and lower-level features, while later layers capture more complex semantics and task-specific features.\n",
    "\n",
    "**Approach**\n",
    "1. Extract the attention weights for the last transformer layer and head. \n",
    "\n",
    "2. Aggregate the attention weights across all heads.\n",
    "\n",
    "3. Visualize the attention distribution to understand which tokens are receiving the most attention.\n",
    "\n",
    "4. Correlate the tokens with high attention with the input features (e.g., specific words in the sentence) to hypothesize which features are influential.\n",
    "\n",
    "### Drawbacks\n",
    "(Hao, Y., Dong, L., Wei, F., & Xu, K. (2021, May). Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 14, pp. 12963-12971))\n",
    "\n",
    "- We observe that the attention score matrix is quite dense\n",
    "\n",
    "- Difficult to understand the interaction between different words.\n",
    "\n",
    "- Even if the attention score is large ,it doesn't mean pair of words is important for model decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1713377889823
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def get_prediction_attention(sentence):\n",
    "    inputs = custom_tokenizer([sentence])\n",
    "    \n",
    "    input_ids = torch.tensor(inputs['input_ids'], dtype=torch.long).to(device)\n",
    "    \n",
    "    attention_mask = torch.tensor([[1]*len(input_ids[0])], dtype=torch.long).to(device)\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).cpu().numpy()[0]  \n",
    "    predicted_prob = probs[0, predicted_class].item()  \n",
    "\n",
    "    return predicted_class, predicted_prob, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1713377891103
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_attention(input_text, attentions):\n",
    "    tokenized_input = custom_tokenizer([input_text])['input_ids'][0] \n",
    "    \n",
    "    tokens = [id_to_token[id] for id in tokenized_input]\n",
    "    \n",
    "    attention = attentions[-1].squeeze(0).mean(0)  \n",
    "    \n",
    "    attention_np = attention.cpu().detach().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention_np, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Interpretation Approach 2. Word Association Extraction\n",
    "This module assesses whether these closely linked tokens exhibit a similar association level in the normal data from the training dataset. To facilitate this analysis, we construct an association matrix M, which encapsulates\n",
    "the average token associations across all normal training data. This matrix serves as a benchmark for comparing associations in the test sentence. If a closely associated token pair from the test sentence also shows a high association in M, no action is taken. However, if their association in the test sentence deviates from the patterns\n",
    "observed in M‚Äîindicating an anomaly‚Äîthis discrepancy is flagged as a violation of an association rule. Such violations, highlighting divergent token relationships from the established norms, are reported as outputs of the component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1713377891295
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_full_association_matrix(sentences, model, tokenizer):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    token_counts = defaultdict(int)\n",
    "    association_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "    \n",
    "    model.eval()  \n",
    "    model.to(device)  \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer([sentence])  \n",
    "        input_ids = inputs[\"input_ids\"][0]  \n",
    "        \n",
    "        input_ids_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids_tensor, output_attentions=True)\n",
    "        attention = outputs.attentions[-1].squeeze(0).mean(dim=0)  \n",
    "        \n",
    "        for i, id1 in enumerate(input_ids):\n",
    "            for j, id2 in enumerate(input_ids):\n",
    "                if id1 < vocab_size and id2 < vocab_size:  \n",
    "                    token_counts[id1] += 1\n",
    "                    token_counts[id2] += 1\n",
    "                    association_matrix[id1, id2] += attention[i, j].item()\n",
    "    \n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            total_occurrences = token_counts[i] + token_counts[j]\n",
    "            if total_occurrences > 0:  \n",
    "                association_matrix[i, j] /= total_occurrences\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    return association_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1713377900666
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 12.65 seconds\n",
      "Total time: 12.66 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "normal_df=df[df['Class']==0]\n",
    "normal_sentences = normal_df['sentence'].tolist()\n",
    "\n",
    "M = create_full_association_matrix(normal_sentences, model, custom_tokenizer)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1713377901248
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(M.flatten(), bins=50, log=True)\n",
    "plt.title('Distribution of Association Scores')\n",
    "plt.xlabel('Association Score')\n",
    "plt.ylabel('Frequency (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1713377901472
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5611657e-06\n"
     ]
    }
   ],
   "source": [
    "threshold = np.mean(M.flatten())\n",
    "violation_threshold=threshold\n",
    "print(violation_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Identify Highly Associated Words**\n",
    "\n",
    "The identify_high_attention_pairs function aims to find pairs of tokens in an input row that receive high attention weights, indicating a strong association according to the model‚Äôs attention mechanism. This module analyzes the attention matrix from the previous module to identify these pairs to return token pairs with attention weights above a specified threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1713377901645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def identify_high_attention_pairs(attention_matrix, tokens):\n",
    "    high_attention_pairs = []\n",
    "    seq_len = attention_matrix.shape[0]\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        for j in range(i + 1, seq_len):  \n",
    "            if attention_matrix[i, j] > threshold:  \n",
    "                token1, token2 = tokens[i], tokens[j]\n",
    "                if token1 not in ['[PAD]', '[SEP]', '[CLS]'] and token2 not in ['[PAD]', '[SEP]', '[CLS]']:  # Filter out special tokens if present\n",
    "                    high_attention_pairs.append((token1, token2))\n",
    "    \n",
    "    return high_attention_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1713377901835
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def process_test_sentence(sentence):\n",
    "    predicted_class, predicted_prob, attentions = get_prediction_attention(sentence)\n",
    "    inputs = custom_tokenizer([sentence])\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    tokens = [id_to_token.get(id, \"<UNK>\") for id in token_ids]  \n",
    "    \n",
    "    if predicted_class == 0 and not (\"<OOV>\" in tokens):\n",
    "        return \"normal\", []\n",
    "\n",
    "    attention_matrix = attentions[-1].squeeze(0).mean(dim=0).cpu().numpy()\n",
    "    \n",
    "    high_attention_pairs = identify_high_attention_pairs(attention_matrix, tokens)\n",
    "    \n",
    "    return \"anomalous\", high_attention_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Check for association rule violations**\n",
    "\n",
    "After identifying high-attention pairs in a test row deemed anomalous, this module compare these pairs against the association matrix ùëÄ to identify rule violations based on a threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gather": {
     "logged": 1713377902165
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def check_association_violations(high_attention_pairs, association_matrix, violation_threshold=violation_threshold):\n",
    "    violations = []\n",
    "    matrix_size = vocab_size\n",
    "\n",
    "    for word1, word2 in high_attention_pairs:\n",
    "        id1 = vocab.get(word1, vocab[\"<OOV>\"])  \n",
    "        id2 = vocab.get(word2, vocab[\"<OOV>\"])\n",
    "\n",
    "        if id1 < matrix_size and id2 < matrix_size:\n",
    "            if association_matrix[id1, id2] < violation_threshold or word1 == \"<OOV>\" or word2 == \"<OOV>\":\n",
    "                violations.append((word1, word2))\n",
    "        else:\n",
    "            print(f\"Skipping pair ({word1}, {word2}) with IDs ({id1}, {id2}) outside association matrix bounds.\")\n",
    "\n",
    "    return violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Mutation Analysis\n",
    "To perform mutation analysis on the dataset and calculate the mutation score, we follow these steps. The code will select 5% of the normal data, randomly mutate two column values per row, and then check if these mutations lead to the data being classified as anomalous and whether the mutated pairs are in the set of violations. The mutation score is calculated as the ratio of mutants killed (those that result in anomalies with violations) to the total number of mutants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gather": {
     "logged": 1713377902805
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: normal\n",
      "Sentence is normal.\n",
      "Status: normal\n",
      "Sentence is normal.\n",
      "[Madh:2]\n",
      "[Madh:2]\n",
      "Status: anomalous\n",
      "Highly associated words based on attention: [('[CT:1]', '[UCSi:7]'), ('[CT:1]', '[UCSh:7]'), ('[CT:1]', '<OOV>'), ('[CT:1]', '[SECS:8]'), ('[CT:1]', '[BN:5]'), ('[CT:1]', '[BC:7]'), ('[CT:1]', '[NN:1]'), ('[CT:1]', '[Mi:3]'), ('[UCSi:7]', '[UCSh:7]'), ('[UCSi:7]', '<OOV>'), ('[UCSi:7]', '[SECS:8]'), ('[UCSi:7]', '[BN:5]'), ('[UCSi:7]', '[BC:7]'), ('[UCSi:7]', '[NN:1]'), ('[UCSi:7]', '[Mi:3]'), ('[UCSh:7]', '<OOV>'), ('[UCSh:7]', '[SECS:8]'), ('[UCSh:7]', '[BN:5]'), ('[UCSh:7]', '[BC:7]'), ('[UCSh:7]', '[NN:1]'), ('[UCSh:7]', '[Mi:3]'), ('<OOV>', '[SECS:8]'), ('<OOV>', '[BN:5]'), ('<OOV>', '[BC:7]'), ('<OOV>', '[NN:1]'), ('<OOV>', '[Mi:3]'), ('[SECS:8]', '[BN:5]'), ('[SECS:8]', '[BC:7]'), ('[SECS:8]', '[NN:1]'), ('[SECS:8]', '[Mi:3]'), ('[BN:5]', '[BC:7]'), ('[BN:5]', '[NN:1]'), ('[BN:5]', '[Mi:3]'), ('[BC:7]', '[NN:1]'), ('[BC:7]', '[Mi:3]'), ('[NN:1]', '[Mi:3]')]\n",
      "Violations of association rules found: [('[CT:1]', '<OOV>'), ('[UCSi:7]', '<OOV>'), ('[UCSh:7]', '<OOV>'), ('<OOV>', '[SECS:8]'), ('<OOV>', '[BN:5]'), ('<OOV>', '[BC:7]'), ('<OOV>', '[NN:1]'), ('<OOV>', '[Mi:3]')]\n",
      "[CT:2]\n",
      "[CT:2]\n",
      "Status: anomalous\n",
      "Highly associated words based on attention: [('<OOV>', '[UCSi:1]'), ('<OOV>', '[UCSh:1]'), ('<OOV>', '[Madh:10]'), ('<OOV>', '[SECS:6]'), ('<OOV>', '[BN:10]'), ('<OOV>', '[BC:1]'), ('<OOV>', '[NN:8]'), ('<OOV>', '[Mi:1]'), ('[UCSi:1]', '[UCSh:1]'), ('[UCSi:1]', '[Madh:10]'), ('[UCSi:1]', '[SECS:6]'), ('[UCSi:1]', '[BN:10]'), ('[UCSi:1]', '[BC:1]'), ('[UCSi:1]', '[NN:8]'), ('[UCSi:1]', '[Mi:1]'), ('[UCSh:1]', '[Madh:10]'), ('[UCSh:1]', '[SECS:6]'), ('[UCSh:1]', '[BN:10]'), ('[UCSh:1]', '[BC:1]'), ('[UCSh:1]', '[NN:8]'), ('[UCSh:1]', '[Mi:1]'), ('[Madh:10]', '[SECS:6]'), ('[Madh:10]', '[BN:10]'), ('[Madh:10]', '[BC:1]'), ('[Madh:10]', '[NN:8]'), ('[Madh:10]', '[Mi:1]'), ('[SECS:6]', '[BN:10]'), ('[SECS:6]', '[BC:1]'), ('[SECS:6]', '[NN:8]'), ('[SECS:6]', '[Mi:1]'), ('[BN:10]', '[BC:1]'), ('[BN:10]', '[NN:8]'), ('[BN:10]', '[Mi:1]'), ('[BC:1]', '[NN:8]'), ('[BC:1]', '[Mi:1]'), ('[NN:8]', '[Mi:1]')]\n",
      "Violations of association rules found: [('<OOV>', '[UCSi:1]'), ('<OOV>', '[UCSh:1]'), ('<OOV>', '[Madh:10]'), ('<OOV>', '[SECS:6]'), ('<OOV>', '[BN:10]'), ('<OOV>', '[BC:1]'), ('<OOV>', '[NN:8]'), ('<OOV>', '[Mi:1]')]\n",
      "Status: normal\n",
      "Sentence is normal.\n",
      "[UCSh:2]\n",
      "[SECS:2]\n",
      "[UCSh:2]\n",
      "[SECS:2]\n",
      "Status: anomalous\n",
      "Highly associated words based on attention: [('[CT:5]', '[UCSi:3]'), ('[CT:5]', '<OOV>'), ('[CT:5]', '[Madh:1]'), ('[CT:5]', '<OOV>'), ('[CT:5]', '[BN:10]'), ('[CT:5]', '[BC:1]'), ('[CT:5]', '[NN:9]'), ('[CT:5]', '[Mi:1]'), ('[UCSi:3]', '<OOV>'), ('[UCSi:3]', '[Madh:1]'), ('[UCSi:3]', '<OOV>'), ('[UCSi:3]', '[BN:10]'), ('[UCSi:3]', '[BC:1]'), ('[UCSi:3]', '[NN:9]'), ('[UCSi:3]', '[Mi:1]'), ('<OOV>', '[Madh:1]'), ('<OOV>', '<OOV>'), ('<OOV>', '[BN:10]'), ('<OOV>', '[BC:1]'), ('<OOV>', '[NN:9]'), ('<OOV>', '[Mi:1]'), ('[Madh:1]', '<OOV>'), ('[Madh:1]', '[BN:10]'), ('[Madh:1]', '[BC:1]'), ('[Madh:1]', '[NN:9]'), ('[Madh:1]', '[Mi:1]'), ('<OOV>', '[BN:10]'), ('<OOV>', '[BC:1]'), ('<OOV>', '[NN:9]'), ('<OOV>', '[Mi:1]'), ('[BN:10]', '[BC:1]'), ('[BN:10]', '[NN:9]'), ('[BN:10]', '[Mi:1]'), ('[BC:1]', '[NN:9]'), ('[BC:1]', '[Mi:1]'), ('[NN:9]', '[Mi:1]')]\n",
      "Violations of association rules found: [('[CT:5]', '<OOV>'), ('[CT:5]', '<OOV>'), ('[UCSi:3]', '<OOV>'), ('[UCSi:3]', '<OOV>'), ('<OOV>', '[Madh:1]'), ('<OOV>', '<OOV>'), ('<OOV>', '[BN:10]'), ('<OOV>', '[BC:1]'), ('<OOV>', '[NN:9]'), ('<OOV>', '[Mi:1]'), ('[Madh:1]', '<OOV>'), ('<OOV>', '[BN:10]'), ('<OOV>', '[BC:1]'), ('<OOV>', '[NN:9]'), ('<OOV>', '[Mi:1]')]\n",
      "[CT:2]\n",
      "[CT:2]\n",
      "Status: anomalous\n",
      "Highly associated words based on attention: [('<OOV>', '[UCSi:1]'), ('<OOV>', '[UCSh:7]'), ('<OOV>', '[Madh:0]'), ('<OOV>', '[SECS:0]'), ('<OOV>', '[BN:1]'), ('<OOV>', '[BC:6]'), ('<OOV>', '[NN:1]'), ('<OOV>', '[Mi:1]'), ('[UCSi:1]', '[UCSh:7]'), ('[UCSi:1]', '[Madh:0]'), ('[UCSi:1]', '[SECS:0]'), ('[UCSi:1]', '[BN:1]'), ('[UCSi:1]', '[BC:6]'), ('[UCSi:1]', '[NN:1]'), ('[UCSi:1]', '[Mi:1]'), ('[UCSh:7]', '[Madh:0]'), ('[UCSh:7]', '[SECS:0]'), ('[UCSh:7]', '[BN:1]'), ('[UCSh:7]', '[BC:6]'), ('[UCSh:7]', '[NN:1]'), ('[UCSh:7]', '[Mi:1]'), ('[Madh:0]', '[SECS:0]'), ('[Madh:0]', '[BN:1]'), ('[Madh:0]', '[BC:6]'), ('[Madh:0]', '[NN:1]'), ('[Madh:0]', '[Mi:1]'), ('[SECS:0]', '[BN:1]'), ('[SECS:0]', '[BC:6]'), ('[SECS:0]', '[NN:1]'), ('[SECS:0]', '[Mi:1]'), ('[BN:1]', '[BC:6]'), ('[BN:1]', '[NN:1]'), ('[BN:1]', '[Mi:1]'), ('[BC:6]', '[NN:1]'), ('[BC:6]', '[Mi:1]'), ('[NN:1]', '[Mi:1]')]\n",
      "Violations of association rules found: [('<OOV>', '[UCSi:1]'), ('<OOV>', '[UCSh:7]'), ('<OOV>', '[Madh:0]'), ('<OOV>', '[SECS:0]'), ('<OOV>', '[BN:1]'), ('<OOV>', '[BC:6]'), ('<OOV>', '[NN:1]'), ('<OOV>', '[Mi:1]'), ('[BN:1]', '[BC:6]')]\n",
      "[CT:2]\n",
      "[CT:2]\n",
      "Status: anomalous\n",
      "Highly associated words based on attention: [('<OOV>', '[UCSi:1]'), ('<OOV>', '[UCSh:3]'), ('<OOV>', '[Madh:10]'), ('<OOV>', '[SECS:1]'), ('<OOV>', '[BN:10]'), ('<OOV>', '[BC:10]'), ('<OOV>', '[NN:1]'), ('<OOV>', '[Mi:1]'), ('[UCSi:1]', '[UCSh:3]'), ('[UCSi:1]', '[Madh:10]'), ('[UCSi:1]', '[SECS:1]'), ('[UCSi:1]', '[BN:10]'), ('[UCSi:1]', '[BC:10]'), ('[UCSi:1]', '[NN:1]'), ('[UCSi:1]', '[Mi:1]'), ('[UCSh:3]', '[Madh:10]'), ('[UCSh:3]', '[SECS:1]'), ('[UCSh:3]', '[BN:10]'), ('[UCSh:3]', '[BC:10]'), ('[UCSh:3]', '[NN:1]'), ('[UCSh:3]', '[Mi:1]'), ('[Madh:10]', '[SECS:1]'), ('[Madh:10]', '[BN:10]'), ('[Madh:10]', '[BC:10]'), ('[Madh:10]', '[NN:1]'), ('[Madh:10]', '[Mi:1]'), ('[SECS:1]', '[BN:10]'), ('[SECS:1]', '[BC:10]'), ('[SECS:1]', '[NN:1]'), ('[SECS:1]', '[Mi:1]'), ('[BN:10]', '[BC:10]'), ('[BN:10]', '[NN:1]'), ('[BN:10]', '[Mi:1]'), ('[BC:10]', '[NN:1]'), ('[BC:10]', '[Mi:1]'), ('[NN:1]', '[Mi:1]')]\n",
      "Violations of association rules found: [('<OOV>', '[UCSi:1]'), ('<OOV>', '[UCSh:3]'), ('<OOV>', '[Madh:10]'), ('<OOV>', '[SECS:1]'), ('<OOV>', '[BN:10]'), ('<OOV>', '[BC:10]'), ('<OOV>', '[NN:1]'), ('<OOV>', '[Mi:1]')]\n",
      "Status: normal\n",
      "Sentence is normal.\n",
      "[UCSi:2]\n",
      "[Madh:2]\n",
      "[UCSi:2]\n",
      "[Madh:2]\n",
      "Status: anomalous\n",
      "Highly associated words based on attention: [('[CT:5]', '<OOV>'), ('[CT:5]', '[UCSh:10]'), ('[CT:5]', '<OOV>'), ('[CT:5]', '[SECS:5]'), ('[CT:5]', '[BN:5]'), ('[CT:5]', '[BC:7]'), ('[CT:5]', '[NN:10]'), ('[CT:5]', '[Mi:1]'), ('<OOV>', '[UCSh:10]'), ('<OOV>', '<OOV>'), ('<OOV>', '[SECS:5]'), ('<OOV>', '[BN:5]'), ('<OOV>', '[BC:7]'), ('<OOV>', '[NN:10]'), ('<OOV>', '[Mi:1]'), ('[UCSh:10]', '<OOV>'), ('[UCSh:10]', '[SECS:5]'), ('[UCSh:10]', '[BN:5]'), ('[UCSh:10]', '[BC:7]'), ('[UCSh:10]', '[NN:10]'), ('[UCSh:10]', '[Mi:1]'), ('<OOV>', '[SECS:5]'), ('<OOV>', '[BN:5]'), ('<OOV>', '[BC:7]'), ('<OOV>', '[NN:10]'), ('<OOV>', '[Mi:1]'), ('[SECS:5]', '[BN:5]'), ('[SECS:5]', '[BC:7]'), ('[SECS:5]', '[NN:10]'), ('[SECS:5]', '[Mi:1]'), ('[BN:5]', '[BC:7]'), ('[BN:5]', '[NN:10]'), ('[BN:5]', '[Mi:1]'), ('[BC:7]', '[NN:10]'), ('[BC:7]', '[Mi:1]'), ('[NN:10]', '[Mi:1]')]\n",
      "Violations of association rules found: [('[CT:5]', '<OOV>'), ('[CT:5]', '<OOV>'), ('<OOV>', '[UCSh:10]'), ('<OOV>', '<OOV>'), ('<OOV>', '[SECS:5]'), ('<OOV>', '[BN:5]'), ('<OOV>', '[BC:7]'), ('<OOV>', '[NN:10]'), ('<OOV>', '[Mi:1]'), ('[UCSh:10]', '<OOV>'), ('<OOV>', '[SECS:5]'), ('<OOV>', '[BN:5]'), ('<OOV>', '[BC:7]'), ('<OOV>', '[NN:10]'), ('<OOV>', '[Mi:1]')]\n",
      "[UCSh:2]\n",
      "[BN:2]\n",
      "[UCSh:2]\n",
      "[BN:2]\n",
      "Status: anomalous\n",
      "Highly associated words based on attention: [('[CT:10]', '[UCSi:1]'), ('[CT:10]', '<OOV>'), ('[CT:10]', '[Madh:1]'), ('[CT:10]', '[SECS:5]'), ('[CT:10]', '<OOV>'), ('[CT:10]', '[BC:7]'), ('[CT:10]', '[NN:1]'), ('[CT:10]', '[Mi:1]'), ('[UCSi:1]', '<OOV>'), ('[UCSi:1]', '[Madh:1]'), ('[UCSi:1]', '[SECS:5]'), ('[UCSi:1]', '<OOV>'), ('[UCSi:1]', '[BC:7]'), ('[UCSi:1]', '[NN:1]'), ('[UCSi:1]', '[Mi:1]'), ('<OOV>', '[Madh:1]'), ('<OOV>', '[SECS:5]'), ('<OOV>', '<OOV>'), ('<OOV>', '[BC:7]'), ('<OOV>', '[NN:1]'), ('<OOV>', '[Mi:1]'), ('[Madh:1]', '[SECS:5]'), ('[Madh:1]', '<OOV>'), ('[Madh:1]', '[BC:7]'), ('[Madh:1]', '[NN:1]'), ('[Madh:1]', '[Mi:1]'), ('[SECS:5]', '<OOV>'), ('[SECS:5]', '[BC:7]'), ('[SECS:5]', '[NN:1]'), ('[SECS:5]', '[Mi:1]'), ('<OOV>', '[BC:7]'), ('<OOV>', '[NN:1]'), ('<OOV>', '[Mi:1]'), ('[BC:7]', '[NN:1]'), ('[BC:7]', '[Mi:1]'), ('[NN:1]', '[Mi:1]')]\n",
      "Violations of association rules found: [('[CT:10]', '<OOV>'), ('[CT:10]', '<OOV>'), ('[UCSi:1]', '<OOV>'), ('[UCSi:1]', '<OOV>'), ('<OOV>', '[Madh:1]'), ('<OOV>', '[SECS:5]'), ('<OOV>', '<OOV>'), ('<OOV>', '[BC:7]'), ('<OOV>', '[NN:1]'), ('<OOV>', '[Mi:1]'), ('[Madh:1]', '<OOV>'), ('[SECS:5]', '<OOV>'), ('<OOV>', '[BC:7]'), ('<OOV>', '[NN:1]'), ('<OOV>', '[Mi:1]')]\n",
      "[UCSh:2]\n",
      "[UCSh:2]\n",
      "Status: anomalous\n",
      "Highly associated words based on attention: [('[CT:8]', '[UCSi:10]'), ('[CT:8]', '<OOV>'), ('[CT:8]', '[Madh:10]'), ('[CT:8]', '[SECS:6]'), ('[CT:8]', '[BN:1]'), ('[CT:8]', '[BC:10]'), ('[CT:8]', '[NN:10]'), ('[CT:8]', '[Mi:10]'), ('[UCSi:10]', '<OOV>'), ('[UCSi:10]', '[Madh:10]'), ('[UCSi:10]', '[SECS:6]'), ('[UCSi:10]', '[BN:1]'), ('[UCSi:10]', '[BC:10]'), ('[UCSi:10]', '[NN:10]'), ('[UCSi:10]', '[Mi:10]'), ('<OOV>', '[Madh:10]'), ('<OOV>', '[SECS:6]'), ('<OOV>', '[BN:1]'), ('<OOV>', '[BC:10]'), ('<OOV>', '[NN:10]'), ('<OOV>', '[Mi:10]'), ('[Madh:10]', '[SECS:6]'), ('[Madh:10]', '[BN:1]'), ('[Madh:10]', '[BC:10]'), ('[Madh:10]', '[NN:10]'), ('[Madh:10]', '[Mi:10]'), ('[SECS:6]', '[BN:1]'), ('[SECS:6]', '[BC:10]'), ('[SECS:6]', '[NN:10]'), ('[SECS:6]', '[Mi:10]'), ('[BN:1]', '[BC:10]'), ('[BN:1]', '[NN:10]'), ('[BN:1]', '[Mi:10]'), ('[BC:10]', '[NN:10]'), ('[BC:10]', '[Mi:10]'), ('[NN:10]', '[Mi:10]')]\n",
      "Violations of association rules found: [('[CT:8]', '<OOV>'), ('[UCSi:10]', '<OOV>'), ('<OOV>', '[Madh:10]'), ('<OOV>', '[SECS:6]'), ('<OOV>', '[BN:1]'), ('<OOV>', '[BC:10]'), ('<OOV>', '[NN:10]'), ('<OOV>', '[Mi:10]')]\n",
      "Mutation Score for anomaly detection: 66.67%\n",
      "Mutation Score for violation detection: 66.67%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "normal_data = df[df['Class'] == 0].sample(frac=0.05)\n",
    "\n",
    "def mutate_sentence(sentence):\n",
    "    columns = sentence.split('],[')\n",
    "    if columns:\n",
    "        columns[0] = columns[0][1:]  \n",
    "        columns[-1] = columns[-1][:-1]  \n",
    "    \n",
    "    mutation_indices = random.sample(range(len(columns)), 2)\n",
    "    mutated_columns = [] \n",
    "    \n",
    "    for idx in mutation_indices:\n",
    "        col_val_split = columns[idx].split(':')\n",
    "        col = col_val_split[0]\n",
    "        mutated_val = str(random.randint(1,2))\n",
    "        \n",
    "\n",
    "        columns[idx] = f\"{col}:{mutated_val}\"\n",
    "        mutated_columns.append(f\"[{col}:{mutated_val}]\")  \n",
    "    \n",
    "    mutated_sentence = '[' + '],['.join(columns) + ']'\n",
    "    return mutated_sentence, mutated_columns\n",
    "\n",
    "killed_mutants_1 = 0\n",
    "killed_mutants_2 = 0\n",
    "total_mutants = len(normal_data) \n",
    "for sentence in normal_data['sentence']:\n",
    "    mutated_sentence, mutated_columns = mutate_sentence(sentence)\n",
    "\n",
    "    status, high_attention_pairs = process_test_sentence(mutated_sentence)\n",
    "    print(f\"Status: {status}\")\n",
    "    if status == \"anomalous\":\n",
    "        print(\"Highly associated words based on attention:\", high_attention_pairs)\n",
    "        killed_mutants_1 += 1\n",
    "        violations = check_association_violations(high_attention_pairs, M)\n",
    "        if violations:\n",
    "            print(\"Violations of association rules found:\", violations)\n",
    "            if any(\"<OOV>\" in pair for violation in violations for pair in violation) or any(col in pair for violation in violations for pair in violation for col in mutated_columns):\n",
    "               killed_mutants_2 += 1\n",
    "\n",
    "        else:\n",
    "            print(\"No violations of association rules.\")\n",
    "    else:\n",
    "        print(\"Sentence is normal.\")\n",
    "\n",
    "mutation_score_1 = (killed_mutants_1 / total_mutants) * 100\n",
    "mutation_score_2 = (killed_mutants_2 / total_mutants) * 100\n",
    "print(f\"Mutation Score for anomaly detection: {mutation_score_1:.2f}%\")\n",
    "print(f\"Mutation Score for violation detection: {mutation_score_2:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Compare with other models\n",
    "\n",
    "Our study involves a comprehensive comparison of the proposed model against traditional anomaly detection methods, including Long Short-Term Memory (LSTM) networks and Multi-Layer Perceptrons (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "gather": {
     "logged": 1713377907031
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uriel/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:89: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6264 - loss: 0.6302 - val_accuracy: 0.7581 - val_loss: 0.3994\n",
      "Epoch 2/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8807 - loss: 0.2754 - val_accuracy: 0.9677 - val_loss: 0.1386\n",
      "Epoch 3/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9535 - loss: 0.1360 - val_accuracy: 0.9677 - val_loss: 0.1240\n",
      "Epoch 4/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9602 - loss: 0.1182 - val_accuracy: 0.9839 - val_loss: 0.0696\n",
      "Epoch 5/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9523 - loss: 0.1489 - val_accuracy: 0.9677 - val_loss: 0.1402\n",
      "Epoch 6/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9653 - loss: 0.1058 - val_accuracy: 0.9677 - val_loss: 0.0996\n",
      "Epoch 7/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9657 - loss: 0.1025 - val_accuracy: 0.9677 - val_loss: 0.0775\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Time taken: 4.27 seconds\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "max_words = 5000  \n",
    "max_len = 50  \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=max_words, output_dim=128, input_shape=(max_len,)))\n",
    "lstm_model.add(LSTM(64))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "lstm_model.fit(padded_sequences, train_labels, batch_size=32, epochs=10, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
    "val_padded_sequences = pad_sequences(val_sequences, maxlen=max_len)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "lstm_preds = (lstm_model.predict(val_padded_sequences) > 0.5).astype('int32').flatten()\n",
    "\n",
    "lstm_accuracy = accuracy_score(val_labels, lstm_preds)\n",
    "lstm_f1 = f1_score(val_labels, lstm_preds, average='micro')\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Time taken: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "gather": {
     "logged": 1713377910221
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uriel/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4253 - loss: 0.7822 - val_accuracy: 0.9677 - val_loss: 0.4759\n",
      "Epoch 2/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8325 - loss: 0.5005 - val_accuracy: 0.9677 - val_loss: 0.3074\n",
      "Epoch 3/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9140 - loss: 0.3601 - val_accuracy: 0.9677 - val_loss: 0.2110\n",
      "Epoch 4/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9212 - loss: 0.2726 - val_accuracy: 0.9677 - val_loss: 0.1521\n",
      "Epoch 5/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9524 - loss: 0.1989 - val_accuracy: 0.9677 - val_loss: 0.1247\n",
      "Epoch 6/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9529 - loss: 0.1658 - val_accuracy: 0.9677 - val_loss: 0.1153\n",
      "Epoch 7/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9635 - loss: 0.1567 - val_accuracy: 0.9677 - val_loss: 0.1131\n",
      "Epoch 8/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9603 - loss: 0.1625 - val_accuracy: 0.9677 - val_loss: 0.1144\n",
      "Epoch 9/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9731 - loss: 0.1087 - val_accuracy: 0.9677 - val_loss: 0.1185\n",
      "Epoch 10/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9687 - loss: 0.1221 - val_accuracy: 0.9677 - val_loss: 0.1248\n",
      "Epoch 11/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9720 - loss: 0.1257 - val_accuracy: 0.9677 - val_loss: 0.1301\n",
      "Epoch 12/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9782 - loss: 0.1066 - val_accuracy: 0.9677 - val_loss: 0.1308\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "Time Taken:1.23 seconds\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X = df.drop(['Class','sentence'], axis=1).values\n",
    "y = df['Class'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_standardized, y, test_size=0.1, random_state=42)\n",
    "\n",
    "mlp_model = Sequential()\n",
    "mlp_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "mlp_model.add(Dropout(0.5))\n",
    "mlp_model.add(Dense(32, activation='relu'))\n",
    "mlp_model.add(Dropout(0.5))\n",
    "mlp_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "mlp_model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "mlp_preds = (mlp_model.predict(X_val) > 0.5).astype('int32').flatten()\n",
    "\n",
    "mlp_accuracy = accuracy_score(y_val, mlp_preds)\n",
    "mlp_f1 = f1_score(y_val, mlp_preds, average='micro')\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Time Taken:{total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "gather": {
     "logged": 1713377910428
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.971014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.985507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.971014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Accuracy  F1 Score\n",
       "0  BERT  0.971014  0.971014\n",
       "1  LSTM  0.985507  0.985507\n",
       "2   MLP  0.971014  0.971014"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_dict = {\n",
    "    'Model': ['BERT', 'LSTM','MLP'],\n",
    "    'Accuracy': [bert_accuracy, lstm_accuracy,mlp_accuracy],\n",
    "    'F1 Score': [bert_f1, lstm_f1, mlp_f1]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_dict)\n",
    "comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
